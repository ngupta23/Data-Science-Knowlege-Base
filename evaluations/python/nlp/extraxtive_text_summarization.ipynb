{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraxtive Text Summarization with BERTsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articke Link: https://analyticsindiamag.com/hands-on-guide-to-extractive-text-summarization-with-bertsum/\n",
    "### Repo: https://github.com/dmmiller612/bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Conda env dl_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summarizer import Summarizer\n",
    "from newspaper import fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.\n",
      "\n",
      "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.\n"
     ]
    }
   ],
   "source": [
    "article_url = 'https://github.com/bhoomikamadhukar/NLP/blob/master/corona.txt'\n",
    "article = fulltext(requests.get(article_url).text)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()\n",
    "result = model(article, min_length=20)\n",
    "summary = \"\".join(result)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Models have shown tremendous advancements in syntactic, semantic and linguistic knowledge for downstream tasks. However, that raises an interesting research question — is it possible for them to go beyond pattern recognition and apply common sense for word-sense disambiguation?\n",
      "\n",
      "Thus, to identify if BERT, a large pre-trained NLP model developed by Google, can solve common sense tasks, researchers took a closer look. The researchers from Westlake University and Fudan University, in collaboration with Microsoft Research Asia, discovered how the model computes the structured, common sense knowledge for downstream NLP tasks.\n",
      "\n",
      "According to the researchers, it has been a long-standing debate as to whether pre-trained language models can solve tasks leveraging only a few shallow clues and their common sense of knowledge. To figure that out, researchers used a CommonsenseQA dataset for BERT to solve multiple-choice problems.\n",
      "\n",
      "In this research, the analysts used CONCEPTNET focusing on attention heads for measuring the common sense knowledge in BERT.\n",
      "\n",
      "Also Read: How I Used BERT to Analyse Twitter Data\n",
      "\n",
      "BERT Model Used For CommonSenseQA\n",
      "\n",
      "To facilitate the process, the researchers chose the multiple-choice question answering dataset, which has been built on CONCEPTNET knowledge graph — CommonSenseQA. This dataset is known for comprising a broad set of triples, of the relation pair — source concept, relation, target concept, considering the concept is ‘Bird’, and the reaction type is ‘at location.’\n",
      "\n",
      "Case in point — with one question and five answers (as shown in the figure above) the model would be asked to select one of the answers as an accurate output. To solve this, conventionally the NLP models would score each of the answers based on sentence-level hidden vector, and the one with the highest score would be the output.\n",
      "\n",
      "However, to examine the presence of common sense in BERT, the researchers examined the common sense link between the question and the answer, which is then manually annoyed in the provided data set.\n",
      "\n",
      "The researchers termed the source concept as ‘question concept’ and the target concept as the ‘answer concept.’ With each question [q], there are five answers [a1 … a5], the researchers then linked the question with each response to obtain five concatenated sequences [s1 … s5], respectively.\n",
      "\n",
      "BERT Architecture.\n",
      "\n",
      "Further, researchers used special symbols in each of the sentences — [CLS] in the beginning; [SEP] in between question and answer, and in the end. With BERT having a stacked Transformer layer to codify each sentence and the last layer of the [CLS] token is then used for linear classification and the answer with the highest score is chosen as the accurate output.\n",
      "\n",
      "Also Read: How Syntactic Biases Help BERT To Achieve Better Language Understanding\n",
      "\n",
      "Does BERT Contain Structured Commonsense Knowledge?\n",
      "\n",
      "To facilitate the analysis, the researchers assessed the common sense links using attention weight and respective attributing score. Attention weights are significant while producing next layer representation, but it becomes insufficient to identify the behaviour of the attention head by disregarding the value of the hidden vector. And that’s why researchers added the supplement of attribution scores to interpret the contribution of each input in backpropagation. Both of the values allowed the researchers to understand the common sense link in BERT better.\n",
      "\n",
      "Firstly, the researchers conducted a set of experiments to figure out if BERT can actually capture the common sense knowledge. According to the paper, it can only be determined “if the link weight from the answer accepts the question concept if higher than the answer concept to other words of the question.”\n",
      "\n",
      "Secondly, to evaluate link weights, the researchers calculated the most associated word with maximum link weights for each of the attention heads in the layers. For this, average accuracy among all attention heads was measured as well as the accuracy of the most accurate head.\n",
      "\n",
      "The figure shows the average and maximum accuracy of the most associated word of BERT for different common-sense relations.\n",
      "\n",
      "Concludingly, if the accuracy of most associated words significantly outstrips the random baseline, it indicates that “relevant question concept plays a significant role in BERT encoding without fine-tuning.” But with fine-tuning when BERT-FT surpasses BERT, it demonstrates that with supervised learning, common sense knowledge can be enhanced in BERT.\n",
      "\n",
      "Also Read: How To Build A BERT Classifier Model With TensorFlow 2.0\n",
      "\n",
      "Wrapping Up\n",
      "\n",
      "Post qualitative and qualitative analysis to understand how the large NLP model, BERT, solves CommonSenseQA tasks. The results denoted that BERT indeed encodes structured commonsense knowledge, and is also able to use the same for certain degrees of downstream NLP tasks. Further researchers have noted that fine-tuning BERT can help enhance this knowledge on higher layers of functions. With the release of the paper, the researchers aim to encourage further leveraging of BERT’s underlying mechanisms for real-world innovations.\n",
      "\n",
      "Read the whole paper here.\n",
      "\n",
      "Provide your comments below\n",
      "\n",
      "comments\n",
      "\n",
      "If you loved this story, do join our Telegram Community.\n",
      "\n",
      "\n",
      "\n",
      "Also, you can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.\n"
     ]
    }
   ],
   "source": [
    "article_url=\"https://analyticsindiamag.com/is-common-sense-common-in-nlp-models/\"\n",
    "article = fulltext(requests.get(article_url).text)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Models have shown tremendous advancements in syntactic, semantic and linguistic knowledge for downstream tasks. Also Read: How I Used BERT to Analyse Twitter Data\n",
      "\n",
      "BERT Model Used For CommonSenseQA\n",
      "\n",
      "To facilitate the process, the researchers chose the multiple-choice question answering dataset, which has been built on CONCEPTNET knowledge graph — CommonSenseQA. However, to examine the presence of common sense in BERT, the researchers examined the common sense link between the question and the answer, which is then manually annoyed in the provided data set. With each question [q], there are five answers [a1 … a5], the researchers then linked the question with each response to obtain five concatenated sequences [s1 … s5], respectively. Both of the values allowed the researchers to understand the common sense link in BERT better. But with fine-tuning when BERT-FT surpasses BERT, it demonstrates that with supervised learning, common sense knowledge can be enhanced in BERT.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()\n",
    "result = model(article, min_length=30, max_length=300)\n",
    "summary = \"\".join(result)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Something else you want to summarize with BERT'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = 'Text body that you want to summarize with BERT'\n",
    "body2 = 'Something else you want to summarize with BERT'\n",
    "model = Summarizer()\n",
    "model(body)\n",
    "model(body2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dl_nlp] *",
   "language": "python",
   "name": "conda-env-.conda-dl_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
